\section{Introduction}\label{sec:intro}
zbMATH classified more than 
\href{https://zbmath.org/?q=%2A+py%3A2019}%
{80k} articles in 2019 according to the Mathematical Subject Classification~(MSC).
With more than
\href{https://msc2020.org}%
{5000} class labels, this classification task requires significant depth-knowledge of the particular fields of mathematics in order to obtain correct fine-grained classifications of the articles.
Therefore the classification is two-fold.
At first, the articles are pre-classified into one of \href{https://msc2020.org}%
{63} main classifications, which are the two first digits of the first MSC label of each article.
In a second step, domain expert assigns fine-grained classification labels in their area of expertise.

In this paper, we focus on the coarse-grained classification and explore how modern machine learning technology can be employed to automate this process.
In particular, we compare the current state of the art technology with a system customized for the application in zbMATH from 2014~\cite{SchonebergS14}.
We formulate the following research questions:
\begin{enumerate}
  \item Which measures are appropriate to measure the quality of automatic classifications?
  \item Does the inclusion of mathematical formulae improve the quality of the classifications?
  \item Does the Part of Speech (POS) preprocessing improve the quality of classifications?
  \item Which features (Title, Abstract, References) are most important for the correct classification?
  \item How does the quality of automatic classification compare to manual classifications?
  \item How to avoid misclassifications in the future?
  \item How can the best performing method be integrated into the current production pipeline at zbMATH?
\end{enumerate}
\section{Method}\label{sec:method}
To investiagte the given problem, we first create a test and training dataset, second we investage different encodings, train our models and evaluate the results.
\subsection{Generation of a test and training dataset}
\subsection{Definition of evaluation metrics}
\subsection{Encoding of the input data}


For our encodings, we prepared the following sources:

\begin{description}
  \item[titls] the titles of the publications.
  \item[texts] the abstract texts of the publications
  \item[refs] the reference mscs of the publications (space separated strings)
  \item[tite] the titles and texts of the publications combined (strings concatenated)
  \item[tiref]: the titles and reference mscs of the publications combined (strings concatenated)
  \item[teref]: the abstract texts and reference mscs of the publications combined (strings concatenated)
  \item[titer]: the titles, and abstract texts, and reference mscs of the publications combined (strings concatenated)
\end{description}





Each of these sources was encoded and classified separately.
\subsection{Clustering}

We encoded our sources using the 'TfidfVectorizer' of the python package Scikit-learn\footnote{\url{https://swmath.org/software/8058}~\cite{swSciKit}}. We used 'utf-8' encoding, and did not perform accent striping or other character normalization except lowercasing. Furthermore, we used the 'word' analyzer without custom stopword list, selecting tokens of two or more alphanumeric characters, processing unigrams, and ignoring punctuation. The resulting vectors consist of float64 entries with 'l2' norm unit output rows.



Our encoder was trained on the trainingset to subsequently transform or vectorize the sources from the testset.

We chose a lightweight 'LogisticRegression' classifier from the python package Scikit-learn. We employed the 'l2' penalty norm with 1e-4 ($10^-4$) tolerance stopping criterion and 1.0 regularization. Furthermore, we allowed intercept constant addition and scaling, but no class weight or custom random state seed. We fitted the classifier using the 'lbfgs' ('Limited-memory BFGS') solver for 100 convergence iterations.
\subsection{Reproducibility}
\url{https://mathscinet.ams.org/mathscinet-getitem?mr=3444005}

\section{Evaluation}\label{sec:eval}

\subsection{pre-Evaluation}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.1\textwidth]{mathEncoding.png}
  \caption{Effect of mathematical symbols on the encoding.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.1\textwidth]{POSeffekt.png}
  \caption{Effect of POS tagging symbols on the encoding.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.1\textwidth]{overview1.png}
  \includegraphics[width=1.1\textwidth]{overview2.png}
  \caption{$F_1$ score of the classification quality of the methods mr1, titer, ref, titls, text grouped by MSC. We use the zbMATH primary MSC as baseline. MSC classes with less than 200 samples were omitted. An interactive version of the figure is available from \url{automsceval.formulasearchengine.com}.}
\end{figure}


\begin{table*}
\begin{tabular}{llll}
  \toprule
  {} &      p &      r &      f \\
  \midrule
  zb1   &      1 &      1 &      1 \\
  mr1   &  0.814 &  0.814 &  0.812 \\
  titer &  0.772 &  0.778 &  0.773 \\
  refs  &  0.748 &  0.753 &  0.746 \\
  titls &  0.637 &  0.627 &  0.623 \\
  texts &  0.699 &  0.709 &  0.699 \\
  ref1  &  0.693 &  0.648 &  0.652 \\
  uT1   &  0.656 &  0.642 &  0.645 \\
  uM1   &  0.655 &  0.639 &  0.644 \\
  tiref &   0.76 &  0.764 &   0.76 \\
  teref &  0.769 &  0.774 &   0.77 \\
  tite  &  0.713 &  0.722 &  0.713 \\
  \bottomrule
\end{tabular}
\begin{tabular}{llll}
  \toprule
  {} &      p &      r &      f \\
  \midrule
  zb1   &  0.817 &  0.807 &   0.81 \\
  mr1   &      1 &      1 &      1 \\
  titer &  0.776 &  0.775 &  0.772 \\
  refs  &  0.743 &  0.743 &  0.737 \\
  titls &  0.644 &  0.632 &  0.627 \\
  texts &  0.704 &  0.709 &  0.699 \\
  ref1  &  0.693 &  0.646 &  0.652 \\
  uT1   &  0.653 &  0.636 &  0.639 \\
  uM1   &  0.652 &  0.632 &  0.636 \\
  tiref &  0.762 &  0.761 &  0.758 \\
  teref &  0.771 &   0.77 &  0.767 \\
  tite  &   0.72 &  0.724 &  0.715 \\
  \bottomrule
\end{tabular}
\caption{Precision $p$, recall $r$ and $F_1$-measure $f$ with regard to the baseline zb1 (left) and mr1 (right).}
\end{table*}


\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{confusion.png}
  \caption{Confusion matrix (titer).}
\end{figure}

\section{Conclusion \& Future Work}\label{sec.concl}
Regarding the research questions, we summarize our findings as follows:
\begin{enumerate}
  \item The quality of the classification for the coarse grained primary MSC can be evaluated with classical information retrieval methods such as precision, recall and accuracy. The downside of binary evaluation approaches is that the degree of incorrectness is not taken into account. We therefore introduced a non-strict evaluation measure that also counts non-primary MSCs as correct.
  \item We did not find evidence that mathematical expressions improve the coarse grained primary classification.
  \item We found that modern NN such as Bert outperform the POS tagging based model developed by \cite{SchonebergS14}. mathematical expressions improve the coarse grained primary classification.
  \item For most classes the reference based approach outperformed the POS tagged based approach.
  \item The manual classification is significantly better for most classes. 
  \item How to avoid misclassifications in the future?
  \item We consider our current model as good enough for the primary MSC pre classification task. We developed an open API which can be accessed from \url{https://automscbackend.formulasearchengine.com}.
\end{enumerate}

\paragraph{Acknowledgments} This work was supported by the German Research Foundation (DFG grant GI 1259-1).
The authors would like to express their gratitude to Felix Hamborg, and Terry Ruas for their advice in the most recent machine learning technology.
\printbibliography[keyword=primary]